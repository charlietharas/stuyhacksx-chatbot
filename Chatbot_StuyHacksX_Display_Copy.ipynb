{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot StuyHacksX Display Copy",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1xCLJYGLkIZMHvW63f6zvRYD-KOOQ-H09",
      "authorship_tag": "ABX9TyMwQc8mel6ZvvEiATDg1Wz2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/charlietharas/stuyhacksx-chatbot/blob/main/Chatbot_StuyHacksX_Display_Copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk8RLzlXb67S"
      },
      "source": [
        "# Preliminary Initialiaztion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uR1qCtL6VMWt"
      },
      "source": [
        "# imports\n",
        "import pandas as pd\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkLV2EOY_ILO"
      },
      "source": [
        "# for reuploads / edits to main datasheet\n",
        "!rm all-merge.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrMQRS04Vaf6"
      },
      "source": [
        "# uploads\n",
        "data_csv = files.upload()\n",
        "filename = \"all-merge.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grw0WRJ2x4t7"
      },
      "source": [
        "# grabbing from drive, because it's nicer this way\n",
        "filename_end = input(\"Filename? \")\n",
        "filename = \"drive/MyDrive/Colab Notebooks/chatbot/\" + filename_end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH9I5byPVXZv"
      },
      "source": [
        "data = pd.read_csv(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gY15YqV5Zhf0"
      },
      "source": [
        "authors = data['Author']\n",
        "content = data['Content']\n",
        "time_diff = data['TimeDiff']\n",
        "conv_id = data['ConvID']\n",
        "is_custom_user = data['IsSpecUser']\n",
        "corpus_id = data['CorpusID']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KngcIqA0lIFC"
      },
      "source": [
        "# added user system for future training off of different people\n",
        "sel_user = input(\"User? \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdJulnEqAn00"
      },
      "source": [
        "Short note about CorpusID: this functionality may be slightly incorrect as it gets 25 distinct databases in all-merge despite there supposedly being 25, meaning that some bases have been incorrectly merged. This **will** require triage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf6z9tIEcD0E"
      },
      "source": [
        "# \"Count\"-Type Analysis\n",
        "\n",
        "e.g. making new features (deriving from current data)\n",
        "These analyses rely on preformatted csv files with additional data semi-manually added.\n",
        "\n",
        "1.   Performs a total message count and analyzes n (eg 100) most used words\n",
        "2.   Counts most prominent authors sending messages prior to those of sel_user (eg most common people conversed with/after)\n",
        "3.   Counts participation in all unique conversations (conversations defined as exchanges where time between messages <30min)\n",
        "4.   Counts distinct frequent groups of people conversed with\n",
        "5.   Counts the amount of words from each user (basic).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA-LC1cxZvm4"
      },
      "source": [
        "usr_message_count = 0\n",
        "total_word_count = 0\n",
        "words = []\n",
        "word_count = []\n",
        "\n",
        "for i in range (content.size):\n",
        "    if str(authors.get(i)) == sel_user:\n",
        "        usr_message_count += 1;\n",
        "        word_in_row = str(content.get(i)).split()\n",
        "        for j in word_in_row:\n",
        "            words.append(j)\n",
        "            total_word_count += 1\n",
        "\n",
        "wordset = set(words)\n",
        "print(\"Total messages from \" + sel_user + \": \" + str(usr_message_count))\n",
        "\n",
        "for i in wordset:\n",
        "    word_count.append([i, words.count(i)])\n",
        "excluded_words = set()\n",
        "most_used_words = []\n",
        "for x in range(100): # bad sorting algorithm\n",
        "    max_i = 0\n",
        "    max_i_word = \"\"\n",
        "    for i in word_count:\n",
        "        if len(i[0]) > 0 and i[0] not in excluded_words:\n",
        "            if i[1] > max_i:\n",
        "                max_i_word = i[0]\n",
        "                max_i = i[1]\n",
        "    excluded_words.add(max_i_word)\n",
        "    most_used_words.append([max_i_word, max_i])\n",
        "\n",
        "print(\"Most used words: \" + str(most_used_words))\n",
        "print(\"Total words: \" + str(total_word_count) + \" at average of \" + str(total_word_count/usr_message_count) + \" wpm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfTH81iZZnpZ"
      },
      "source": [
        "authorlist = []\n",
        "author_count = []\n",
        "\n",
        "for i in range (authors.size):\n",
        "    if str(authors.get(i)) == sel_user:\n",
        "        authorlist.append(authors.get(i-1))\n",
        "\n",
        "authorset = set(authorlist)\n",
        "\n",
        "for i in authorset:\n",
        "    author_count.append([i, authorlist.count(i)])\n",
        "\n",
        "print(\"Authors prior to send count \" + str(author_count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gxtd9v1dd6VL"
      },
      "source": [
        "convset = set()\n",
        "\n",
        "for i in range(conv_id.size):\n",
        "    if str(authors.get(i)) == sel_user:\n",
        "        convset.add(conv_id.get(i))\n",
        "\n",
        "print(\"Got \" + str(int(conv_id.get(conv_id.size-1))) + \" distinct conversations, participation in \" + str(len(convset)) + \" at rate \" + str(len(convset)/int(conv_id.get(conv_id.size-1))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-V3HION7qnmr"
      },
      "source": [
        "authors_permutations = []\n",
        "convID = -1\n",
        "\n",
        "for i in range(conv_id.size):\n",
        "    if conv_id.get(i) in convset:\n",
        "        if conv_id.get(i) != convID:\n",
        "            authors_permutations.append(set())\n",
        "        else:\n",
        "            authors_permutations[len(authors_permutations)-1].add(authors.get(i))\n",
        "        convID = conv_id.get(i)\n",
        "\n",
        "# creates set of permutations (list)\n",
        "authors_permutations_included = []\n",
        "for i in authors_permutations:\n",
        "    if i in authors_permutations_included:\n",
        "        continue\n",
        "    else:\n",
        "        authors_permutations_included.append(i)\n",
        "\n",
        "authors_permutations_count = []\n",
        "for i in authors_permutations_included:\n",
        "    authors_permutations_count.append([i, authors_permutations.count(i)])\n",
        "\n",
        "# get unsorted list of distinct conversational groups\n",
        "print(authors_permutations_count)\n",
        "\n",
        "# bug noticed: prints [set(), 382], but.. whatever"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fme6TSigPRa"
      },
      "source": [
        "sel_user_word_count = 0\r\n",
        "user2_word_count = 0\r\n",
        "user2 = input(\"Select user comparator? \")\r\n",
        "for i in range(content.size):\r\n",
        "    try:\r\n",
        "        split_list = str(content[i]).split(' ')\r\n",
        "    except:\r\n",
        "        print(\"Error at i count\", i)\r\n",
        "    if authors[i] == sel_user:\r\n",
        "        sel_user_word_count += len(split_list)\r\n",
        "    elif authors[i] == user2:\r\n",
        "        user2_word_count += len(split_list)\r\n",
        "\r\n",
        "print(sel_user, \"at\", sel_user_word_count, \"words.\")\r\n",
        "print(user2, \"at\", user2_word_count, \"words.\")\r\n",
        "print(\"Ratio is\", sel_user_word_count/user2_word_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHCdSJ33A2ja"
      },
      "source": [
        "Short noted bug that within authors_permutation there is an entry being [set(), 382], which is problematic but not critical. This may be addressed later.\r\n",
        "\r\n",
        "Important bug usr_message_count is nonfunctional, ignored temporarily. Requires triage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfVQJUhtejJM"
      },
      "source": [
        "# Sequence to Sequence\n",
        "\n",
        "Vectorizing the dictionary of distinct words as a Vocabulary object, grabbing specialized conversation pairs, and training a model to respond. This is the core of the project, inspired by and heavily relying on content from [here](https://medium.com/swlh/end-to-end-chatbot-using-sequence-to-sequence-architecture-e24d137f9c78)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsvLnFFe4qei"
      },
      "source": [
        "# imports for this section\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "import itertools\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXgEfIeNek7C"
      },
      "source": [
        "# defining how a vocabulary object is set up\n",
        "# relies on running above code to get count of distinct words as word_count\n",
        "PAD = 0\n",
        "SRT = 1\n",
        "END = 2\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.trimmed = False\n",
        "        self.word_to_index = {}\n",
        "        self.word_to_count = {}\n",
        "        self.index_to_word = {PAD: \"PAD\", SRT: \"SOS\", END: \"EOS\"}\n",
        "        self.num_words = 3\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWordNoContext(word)\n",
        "\n",
        "    def addWordNoContext(self, word):\n",
        "        if word not in self.word_to_index:\n",
        "            self.word_to_index[word] = self.num_words\n",
        "            self.word_to_count[word] = 1\n",
        "            self.index_to_word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word_to_count[word] += 1\n",
        "\n",
        "    def addWord(self, word, index, count):\n",
        "        self.word_to_index[word] = index\n",
        "        self.word_to_count[word] = count\n",
        "        self.index_to_word[index] = word\n",
        "        self.num_words += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn-nN-Usff7O"
      },
      "source": [
        "# functions to fix bad characters and clean up messages, optimizing convergence\n",
        "def fixASCII(string):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', string) if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "def fixString(string):\n",
        "    string = fixASCII(string.lower().strip())\n",
        "    string = re.sub(r\"([.!?])\", r\" \\1\", string)\n",
        "    string = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", string)\n",
        "    string = re.sub(r\"\\s+\", r\" \", string).strip()\n",
        "    return string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDqXnRQO2v7s"
      },
      "source": [
        "# normalizing words and generating the Vocabulary object for the complete dataset\n",
        "# not actually relevant to final model?\n",
        "print(\"Got\", len(word_count), \"distinct words.\")\n",
        "valid_word_count = []\n",
        "for i in word_count:\n",
        "    if i[0] == fixString(i[0]):\n",
        "        valid_word_count.append(i)\n",
        "\n",
        "print(\"Got\", len(valid_word_count), \"distinct valid words.\")\n",
        "master_voc = Vocabulary(\"all-merge\")\n",
        "for i in range(len(valid_word_count)):\n",
        "    master_voc.addWord(valid_word_count[i][0], i, valid_word_count[i][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hZW1IgR6UE4"
      },
      "source": [
        "## Generating Sentence Pair Objects\n",
        "\n",
        "Various methods of generating objects for sentence pair objects for training the model.\n",
        "This section will also build specific vocabulary objects for each distinct conversation filter.\n",
        "\n",
        "1.   \"Dumb\" grabber between two users. Considers only previous lines, offers little context, and scans the entire corpus: weak for serious training.\n",
        "2.   \"Less dumb\" grabber between selected user for training and any other user. Considers only previous lines, offers little context, and scans the entire corpus. Marginally better than the other but also may offer less clarity/personality because of different interaction patterns between different users.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3qxl2jBh2NI"
      },
      "source": [
        "# get dumb user grabber user\r\n",
        "user = input(\"User for dumb grabber: \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnHUi3Ry7D4A"
      },
      "source": [
        "# \"dumb\" grabber: only contextualizes single line conversation between two distinct users\n",
        "pairs = []\n",
        "vocabulary = Vocabulary(\"Dumb 2-user grabber\")\n",
        "for i in range(1, len(content)):\n",
        "    if authors[i] == sel_user and authors[i-1] == user:\n",
        "        try:\n",
        "            curr_cont = fixString(content[i])\n",
        "            prev_cont = fixString(content[i-1])\n",
        "            pairs.append([prev_cont, curr_cont])\n",
        "            vocabulary.addSentence(curr_cont)\n",
        "            vocabulary.addSentence(prev_cont)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "print(\"Discriminant with 2-user basic filter grabbed\", len(pairs), \"distinct pairs across entire corpus.\")\n",
        "print(\"Corresponding Vocabulary object with\", vocabulary.num_words, \"distinct words.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtB1Xd9jhvUB"
      },
      "source": [
        "# \"less dumb\" grabber: builds pairs out of anyone talking to user\r\n",
        "pairs = []\r\n",
        "vocabulary = Vocabulary(\"Less dumb 2-user grabber\")\r\n",
        "for i in range(1, len(content)):\r\n",
        "    if authors[i] == sel_user and [authors[i-1]] != sel_user:\r\n",
        "        try:\r\n",
        "            curr_cont = fixString(content[i])\r\n",
        "            prev_cont = fixString(content[i-1])\r\n",
        "            pairs.append([prev_cont, curr_cont])\r\n",
        "            vocabulary.addSentence(curr_cont)\r\n",
        "            vocabulary.addSentence(prev_cont)\r\n",
        "        except:\r\n",
        "            continue\r\n",
        "\r\n",
        "print(\"Discriminant with any-user basic filter grabbed\", len(pairs), \"distinct pairs across entire corpus.\")\r\n",
        "print(\"Corresponding Vocabulary object with\", vocabulary.num_words, \"distinct words.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7yFfmHCD3dN"
      },
      "source": [
        "## Data Preparation\n",
        "Preparing batches for use in the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf_ahh-XFO0S"
      },
      "source": [
        "# utility functions\n",
        "# multi-grabs indexes from vocabulary\n",
        "def getIndexesFromSent(voc, sent):\n",
        "    return [voc.word_to_index[word] for word in sent.split(' ')] + [END]\n",
        "\n",
        "# generating padding\n",
        "def genPadding(batch, fillvalue=PAD):\n",
        "    return list(itertools.zip_longest(*batch, fillvalue=fillvalue))\n",
        "\n",
        "# returns binary matrix adjusting for padding\n",
        "def binaryMatrix(batch, value=PAD):\n",
        "    matrix = []\n",
        "    for i, seq in enumerate(batch):\n",
        "        matrix.append([])\n",
        "        for token in seq:\n",
        "            if token == PAD:\n",
        "                matrix[i].append(0)\n",
        "            else:\n",
        "                matrix[i].append(1)\n",
        "    return matrix\n",
        "\n",
        "# padding functions\n",
        "# return input tensor and corresponding lengths\n",
        "def inputVariable(batch, voc):\n",
        "    idxs_batch = [getIndexesFromSent(voc, sentence) for sentence in batch]\n",
        "    lengths = torch.tensor([len(indexes) for indexes in idxs_batch])\n",
        "    padded_list = genPadding(idxs_batch)\n",
        "    padded_variable = torch.LongTensor(padded_list)\n",
        "    return padded_variable, lengths\n",
        "\n",
        "# return target tensor, padding mask, and maximum length\n",
        "def outputVariable(batch, voc):\n",
        "    idxs_batch = [getIndexesFromSent(voc, sentence) for sentence in batch]\n",
        "    max_len = max([len(indexes) for indexes in idxs_batch])\n",
        "    padded_list = genPadding(idxs_batch)\n",
        "    mask = binaryMatrix(padded_list)\n",
        "    mask = torch.ByteTensor(mask)\n",
        "    padded_variable = torch.LongTensor(padded_list)\n",
        "    return padded_variable, mask, max_len\n",
        "\n",
        "# converts batch into train data\n",
        "def batch_to_data(voc, batch):\n",
        "    batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
        "    input_batch = []\n",
        "    output_batch = []\n",
        "    for pair in batch:\n",
        "        input_batch.append(pair[0])\n",
        "        output_batch.append(pair[1])\n",
        "    inpt, lengths = inputVariable(input_batch, voc)\n",
        "    output, mask, max_len = outputVariable(output_batch, voc)\n",
        "    return inpt, lengths, output, mask, max_len\n",
        "\n",
        "# example\n",
        "batches = batch_to_data(vocabulary, [random.choice(pairs) for i in range(5)])\n",
        "input_var, lengths, target_var, mask, max_len = batches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mvic8-F0gq1i"
      },
      "source": [
        "## The Model\n",
        "The model in this case revolves around 3 layers\n",
        "\n",
        "1.   An encoder to losslessly vectorize words into trainable binary sequences (for this we use a bidirectional GRU).\n",
        "2.   An attention layer prioritizes different parts of sentences for \"understanding.\" For this we use a Luong attention layer.\n",
        "3.   A decoder to convert the model's inner \"thoughts\" into output for the user!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJL75ERiZUP-"
      },
      "source": [
        "# tensordash\n",
        "!pip install tensor-dash\n",
        "from tensordash.torchdash import Torchdash\n",
        "histories = Torchdash(ModelName=\"Chatbot\", email=\"charlie@charliemax.dev\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpcagVJChKjL"
      },
      "source": [
        "# encoder\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = embedding\n",
        "\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers==1 else dropout), bidirectional=True)\n",
        "\n",
        "    def forward(self, input_sequence, input_lengths, hidden=None):\n",
        "        embedded = self.embedding(input_sequence)\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
        "        return outputs, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FnCx5h_jF0Z"
      },
      "source": [
        "# attention layer\n",
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        self.method = method\n",
        "        if self.method not in ['dot', 'general', 'concat']:\n",
        "            raise ValueError(self.method, \"is not a valid attention method.\")\n",
        "        self.hidden_size = hidden_size\n",
        "        if self.method == 'general':\n",
        "            self.attn = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "            self.v = nn.Parameter(torch.FloatTensor(self.hidden_size))\n",
        "\n",
        "    def dot_score(self, hidden, encoder_output):\n",
        "        return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(encoder_output)\n",
        "        return torch.sum(hidden * energy, dim=2)\n",
        "\n",
        "    def concat_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(encoder_output)\n",
        "        return torch.sum(hidden * energy, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        if self.method == 'general':\n",
        "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'concat':\n",
        "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'dot':\n",
        "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "        attn_energies = attn_energies.t()\n",
        "\n",
        "        return nn.functional.softmax(attn_energies, dim=1).unsqueeze(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFRdwomJlCA1"
      },
      "source": [
        "# decoder\n",
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
        "        super(LuongAttnDecoderRNN, self).__init__()\n",
        "\n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.embedding = embedding\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, self.n_layers, dropout=(0 if self.n_layers == 1 else dropout))\n",
        "        self.concat = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "        self.attn = Attn(self.attn_model, self.hidden_size)\n",
        "\n",
        "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input_step)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        output = self.out(concat_output)\n",
        "        output = nn.functional.softmax(output, dim=1)\n",
        "        return output, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4h8gDpemnY3"
      },
      "source": [
        "# loss function\n",
        "def loss_func(inpt, target, mask):\n",
        "    n_total = mask.sum()\n",
        "    cross_entropy = -torch.log(torch.gather(inpt, 1, target.view(-1, 1)).squeeze(1))\n",
        "    loss = cross_entropy.masked_select(mask).mean()\n",
        "    loss = loss.to(device)\n",
        "    return loss, n_total.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV51Lw6AoIsv"
      },
      "source": [
        "# training functions\n",
        "device = torch.device(\"cpu\")\n",
        "def train(input_variable, lengths, target_variable, mask, max_len, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip):\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_variable = input_variable.to(device)\n",
        "    lengths = lengths.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    mask = mask.to(device)\n",
        "\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "\n",
        "    decoder_input = torch.LongTensor([[SRT for i in range (batch_size)]])\n",
        "    decoder_input = decoder_input.to(device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_len):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_input = target_variable[t].view(1, -1)\n",
        "            mask_loss, n_total = loss_func(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * n_total)\n",
        "            n_totals += n_total\n",
        "    else:\n",
        "        for t in range(max_len):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "            decoder_input = decoder_input.to(device)\n",
        "            mask_loss, n_total = loss_func(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * n_total)\n",
        "            n_totals += n_total\n",
        "    \n",
        "    loss.backward()\n",
        "\n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return sum(print_losses) / n_totals\n",
        "\n",
        "def train_iterations(model_name, vocabulary, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, n_iterations, batch_size, print_rate, save_rate, clip):\n",
        "    training_batches = [batch_to_data(vocabulary, [random.choice(pairs) for i in range(batch_size)]) for ii in range(n_iterations)]\n",
        "\n",
        "    start_iteration = 1 # should be 1\n",
        "    print_loss = 0\n",
        "    \n",
        "    for iteration in range(start_iteration, n_iterations + 1):\n",
        "        training_batch = training_batches[iteration - 1]\n",
        "        input_variable, lengths, target_variable, mask, max_len = training_batch\n",
        "\n",
        "        loss = train(input_variable, lengths, target_variable, mask, max_len, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
        "        print_loss += loss\n",
        "\n",
        "        # tensordash\n",
        "        histories.sendLoss(loss = loss, epoch = iteration, total_epochs = n_iterations+1)\n",
        "\n",
        "        if iteration % print_rate == 0:\n",
        "            print_loss_avg = print_loss / print_rate\n",
        "            train_loss.append(print_loss_avg)\n",
        "            print(\"Iteration {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration/n_iterations*100, print_loss_avg))\n",
        "            print_loss = 0\n",
        "\n",
        "        if iteration % save_rate == 0:\n",
        "            directory = os.path.join(\"drive/MyDrive/Colab Notebooks/chatbot/saves\", sel_user, model_name, \"all\", '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
        "            if not os.path.exists(directory):\n",
        "                os.makedirs(directory)\n",
        "            torch.save({\n",
        "                'iteration': iteration,\n",
        "                'en': encoder.state_dict(),\n",
        "                'de': decoder.state_dict(),\n",
        "                'en_opt': encoder_optimizer.state_dict(),\n",
        "                'de_opt': decoder_optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                'voc_dict': vocabulary.__dict__,\n",
        "                'embedding': embedding.state_dict()\n",
        "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdAFk1We2xA5"
      },
      "source": [
        "# searcher\n",
        "class GreedySearchDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder, use_multinomial=False):\n",
        "        super(GreedySearchDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.use_multinomial = use_multinomial\n",
        "\n",
        "    def forward(self, input_sequence, input_length, max_len):\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_sequence, input_length)\n",
        "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SRT\n",
        "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
        "        all_scores = torch.zeros([0], device=device)\n",
        "        if not self.use_multinomial:\n",
        "            for i in range(max_len):\n",
        "                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "                decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "                all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "                all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "                decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "            return all_tokens, all_scores\n",
        "        else:\n",
        "            for i in range(max_len):\n",
        "                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "                decoder_output_multi = decoder_output.data.view(-1).div(0.7).exp()\n",
        "                decoder_input = torch.multinomial(decoder_input_multi, 1)\n",
        "                decoder_scores, _ = torch.max(decoder_output, dim=1)\n",
        "                all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "                all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "                decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "            return all_tokens, all_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJFMHsftm5zU"
      },
      "source": [
        "# training the model\n",
        "# params\n",
        "clip = 50.0\n",
        "teacher_forcing = 0.9\n",
        "alpha = 0.0001\n",
        "decoder_learning = 5.0\n",
        "n_iter = 500 # from 500\n",
        "print_rate = 50\n",
        "save_rate = 100\n",
        "teacher_forcing_ratio = 1.0\n",
        "model_name = 'cb_model'\n",
        "attn_model = 'dot'\n",
        "hidden_size = 512\n",
        "encoder_n_layers = 2\n",
        "decoder_n_layers = 2\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "train_loss = []\n",
        "\n",
        "embedding = nn.Embedding(vocabulary.num_words, hidden_size)\n",
        "\n",
        "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, vocabulary.num_words, decoder_n_layers, dropout)\n",
        "\n",
        "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=alpha)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=alpha * decoder_learning)\n",
        "\n",
        "encoder.train()\n",
        "decoder.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkX72CHm5TUo"
      },
      "source": [
        "# the training function\n",
        "train_iterations(model_name, vocabulary, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, n_iter, batch_size, print_rate, save_rate, clip)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGEjQZH4-15E"
      },
      "source": [
        "# loading models\n",
        "spec_filename = \"500_checkpoint.tar\"\n",
        "load_filename = os.path.join(\"drive/MyDrive/Colab Notebooks/chatbot/saves\", sel_user, model_name, \"all\", '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size), spec_filename)\n",
        "checkpoint = torch.load(load_filename)\n",
        "encoder_sd = checkpoint['en']\n",
        "decoder_sd = checkpoint['de']\n",
        "encoder_optimizer_sd = checkpoint['en_opt']\n",
        "decoder_optimizer_sd = checkpoint['de_opt']\n",
        "embedding_sd = checkpoint['embedding']\n",
        "vocabulary.__dict__ = checkpoint['voc_dict']\n",
        "embedding.load_state_dict(embedding_sd)\n",
        "encoder.load_state_dict(encoder_sd)\n",
        "decoder.load_state_dict(decoder_sd)\n",
        "encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
        "decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
        "encoder.to(device)\n",
        "decoder.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvcyy74Q0idx"
      },
      "source": [
        "# evaluation\n",
        "def evaluate(encoder, decoder, searcher, voc, sent, temperature=False):\n",
        "    idxs_batch = [getIndexesFromSent(voc, sent)]\n",
        "    lengths = torch.tensor([len(indexes) for indexes in idxs_batch])\n",
        "    input_batch = torch.LongTensor(idxs_batch).transpose(0, 1)\n",
        "    input_batch = input_batch.to(device)\n",
        "    lengths = lengths.to(device)\n",
        "    tokens, scores = searcher(input_batch, lengths, 12)\n",
        "    decoded_words = [voc.index_to_word[token.item()] for token in tokens]\n",
        "    return decoded_words\n",
        "\n",
        "def do_evaluate(encoder, decoder, searcher, voc):\n",
        "    input_sent = input()\n",
        "    if input_sent == \"exitexit\":\n",
        "        print(\"Quit.\")\n",
        "        exit()\n",
        "    input_sent = fixString(input_sent)\n",
        "    outputs = evaluate(encoder, decoder, searcher, voc, input_sent)\n",
        "    outputs[:] = [x for x in outputs if not (x=='EOS' or x=='PAD')]\n",
        "    print(\"Says:\", ' '.join(outputs))\n",
        "\n",
        "# change these to encoder, decoder when not loading\n",
        "searcher = GreedySearchDecoder(encoder, decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1gHA8f0NeLc"
      },
      "source": [
        "# evaluation when just trained\n",
        "print(\"exitexit to stop.\")\n",
        "while True:\n",
        "    do_evaluate(encoder, decoder, searcher, vocabulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEOyBP6bppCP"
      },
      "source": [
        "# todo:\n",
        "# fix keyerrors for unknown words in input (probably isn't fixable)\n",
        "# triage bugs noted in text comments/fix text comments?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgttmVUixwUP"
      },
      "source": [
        "# Discord Implementation\n",
        "Code for running a discord bot with this model. This code does not run online but can be implemented server-side."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3ZhUxWs-wIv"
      },
      "source": [
        "!pip install discord"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMhUcuO4TBet"
      },
      "source": [
        "TOKEN = input(\"Token: \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ys8ukMBnSdlg"
      },
      "source": [
        "import discord\r\n",
        "\r\n",
        "client = discord.Client()\r\n",
        "\r\n",
        "@client.event\r\n",
        "async def on_ready():\r\n",
        "    print('Logged on as user {0.user}'.format(client))\r\n",
        "\r\n",
        "@client.event\r\n",
        "async def on_message(message):\r\n",
        "    if message.author == client.user:\r\n",
        "        return\r\n",
        "    \r\n",
        "    if message.content.startswith('$hello'):\r\n",
        "        await message.channel.send('Hello!')\r\n",
        "        \r\n",
        "    if message.content.startswith('$hey'):\r\n",
        "        content_str = message.content[4:]\r\n",
        "        try:\r\n",
        "            await message.channel.send(do_evaluate(encoder, decoder, searcher, vocabulary, content_str))\r\n",
        "        except:\r\n",
        "            await message.channel.send('Error, unknown word.')\r\n",
        "        \r\n",
        "client.run(TOKEN)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}